{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isimorfizam/cake/blob/main/2_fraud_detection_exploratory_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwY1xzK00WH7"
      },
      "source": [
        "# FRAUD DETECTION - Exploratoty analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKXeoACnxta7"
      },
      "source": [
        "\n",
        "# 1. Introduction\n",
        "♡\n",
        "\n",
        "We are presented with a dataset of 3016 rows and 50 columns. The rows represent workspaces and their respective attributes, including, but not limited to : workspace_id - identification feature, continent of workspace owner, number of total, activated and deactivated workspace users respectively and a long list of features which quantify Clockify app usage. For a comprehensive list of features, see Section 3. Feature list.\n",
        "\n",
        "The goal of this notebook is to explore the data at hand and try to understand which of the many features available have the greatest impact on the classification. We will use a few different approaches, all of which require a short form of preprocessing, which is done in Section 3.\n",
        "\n",
        "\n",
        "The approaches for establishing feature relevance are listed in Section 4, 5 and 6 and are following :\n",
        "\n",
        "Section 4 - Correlation analysis : Spearman's correlation coefficient used;\n",
        "Section 5 - Decision Tree and Feature importance. We used 2 types of metrics : entropy and Gini index;\n",
        "Section 6 - Information gain;\n",
        "\n",
        "---\n",
        "\n",
        "# SQL Preprocessing - Important encodings\n",
        "\n",
        "Categorical values had already been encoded in SQL. List of categorical values and their encoding patterns goes like this :    \n",
        "1. Country : represented by its continent, then continent encoded by its frequency of appearance. Found in column 'freq'. Frequency encoding goes like this :\n",
        "'America' : 59\n",
        "'Europe' : 26\n",
        "'Asia' : 9\n",
        "'Australia' : 4\n",
        "'Africa' : 2\n",
        "2. Email : 1 if gmail.com, yahoo.com, outlook.com or yandex.com, ELSE 0; found in email_enc\n",
        "3. Account type : 0 if CAKE_INC, 1 if CAKE_AG, 2 if both appeared for the same workspace_id.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Short review of results\n",
        "\n",
        "1. Spearman's coeficient showed that the most correlated features are : email_enc (+0.1) and account_enc_2 (+0.08), freq(-0.08) and project_creation(0.09).\n",
        "2. In both approaches used to build a Decision Tree (Gini index and entropy), it was found that project creation greater than 0.5 means that there was no fraud.\n",
        "3. With project creation lower than 0.5 and email_enc < 0.5, which is email_enc = 0, which is non-standard mail (not gmail, yahoo etc)\n",
        "4. Account_enc_2 < 0.5, which is 0, meaning CAKE_INC account, suggests 'no fraud'\n",
        "5. Information gains between features and target variable suggest most important features are : 'project_creation','max_plan','max_logins', 'freq', 'email_enc', which is supported by other analysis as well.\n",
        "6. Features that are decided as irrelevant based on the dataset we have, and that could be dropped from future usage are following :\n",
        "\n",
        "'gps_tracking',\n",
        " 'time_entries_timesheet',\n",
        " 'trial_activation',\n",
        " 'screenshot_capturing',\n",
        " 'duration_format',\n",
        " 'activate_scheduling',\n",
        " 'publish_schedule',\n",
        " 'connect_calendar',\n",
        " 'deactivated_users',\n",
        " 'invited_users',\n",
        " 'tracked_time_timesheet',\n",
        " 'connect_quickbooks',\n",
        " 'project_budget',\n",
        " 'timesheet_approvals',\n",
        " 'add_role',\n",
        " 'activate_audit_log',\n",
        " 'add_targets',\n",
        " 'create_expense',\n",
        " 'integrations',\n",
        " 'time_entries_calendar',\n",
        " 'max_plan',\n",
        " 'max_share_report',\n",
        " 'create_invoice',\n",
        " 'create_custom_field',\n",
        " 'tracked_time_calendar',\n",
        " 'add_delete_tag',\n",
        " 'lock_entries',\n",
        " 'rounding_reports',\n",
        " 'time_off',\n",
        " 'import_csv'.  These were found as an intersection of features with information gain less than 0.02 and correlation less than 0.02\n",
        "\n",
        " 7. Boruta algorythm's green zone features :\n",
        "  ['activated_users',\n",
        "  'total_users',\n",
        "  'project_creation',\n",
        "  'add_client',\n",
        "  'export_report',\n",
        "  'time_entries_tracker',\n",
        "  'tracked_time_tracker',\n",
        "  'add_new_member',\n",
        "  'time_entries_calendar',\n",
        "  'time_entries_timesheet',\n",
        "  'tracked_time_timesheet',\n",
        "  'add_filter',\n",
        "  'account_enc_2',\n",
        "  'max_logins',\n",
        "  'email_enc',\n",
        "  'days_to_purchase'] and blue zone features :['create_invoice', 'tracked_time_calendar', 'freq']\n",
        "\n",
        "8. Some contradictory results between Boruta and other approaches. Boruta gives more importance to following :    \n",
        "\n",
        " ['time_entries_calendar', 'tracked_time_timesheet', 'add_new_member', 'time_entries_timesheet']\n",
        "['create_invoice', 'tracked_time_calendar']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egLQGSKRzWSG"
      },
      "source": [
        "# Installation and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pWahy1ViCks_",
        "outputId": "ab215df5-03e5-470c-abce-1fa0e02d71e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/357.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/357.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.9/357.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.1/686.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.9/485.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.2/127.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "!pip install --quiet ydata_profiling\n",
        "!pip install --quiet pycaret\n",
        "!pip install --quiet tqdm\n",
        "!pip install --quiet numpy==1.19.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uJENjEBLWqw7"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# open Google Sheets document as CSV\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns',None,'display.max_rows',None)\n",
        "\n",
        "url = 'https://docs.google.com/spreadsheets/d/1TqnAXmB_9JM5c4MRewyE438NculxqsZrnOcQ2Rb-iW0/export?format=csv'\n",
        "df = pd.read_csv(url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijy9-f-ozBf4"
      },
      "source": [
        "# 2. Example of data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPnOeexhLYv8"
      },
      "outputs": [],
      "source": [
        "# Making sure the frame is loaded correctly, getting a preview of the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGQKudD20quC"
      },
      "source": [
        "Check for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "i7VN1_THj4_y"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(df.duplicated().sum())\n",
        "# df.drop_duplicates(inplace=True)\n",
        "# df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMErA11Ay7BJ"
      },
      "source": [
        "# 3. Feature list and preprocessing\n",
        "\n",
        "In this section we deal with missing values, list the features and try to get some understanding of how the features are distributed over the dataset.\n",
        "\n",
        "\n",
        "In the first paragraph we are presented with a list of all features, their data types, and a number of non-null values in each feature, which we use to find and understand missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IXBWiAKlLe_S"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Finding rows with missing values and checking data types\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqd6BinL1PNd"
      },
      "source": [
        "Next, we check the number of unique values for each column and drop columns with only one value, since those bring no relevant information.\n",
        "In this case, we only have one column with a single value across the dataset - the 'duplucates' column. This column was artificially added and it is a bool value : 'True' if the same workspace apeared more that once in any of the tables used for creating the dataset and 'False' otherwise. It is expected for each workspace to have more apearances, ergo, for 'duplicates' feature to always be truthful, since we used tables that mark each action in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Tbb5WrSQ1e"
      },
      "outputs": [],
      "source": [
        "# Checking number of unique values for each column / If ever there was 1, for example, we would drop that column\n",
        "df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpliqw_XMX1M"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['duplicates','cnt'],inplace=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ZvYHZuLuBi"
      },
      "source": [
        "Now, we deal with columns that have null values, aka missing values. Those include : 'tracked_time_calendar', 'tracked_time_timesheet' and 'tracked_time_tracker'. Since these columns are measures of certain events, their activation is triggered when an event happens (when time tracking is done through one of these options). Hence, missing values represent absence of the event, which is equivalent to a duration of 0. And that is how we will represent null."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QovhPdq7Ll0V"
      },
      "outputs": [],
      "source": [
        "df['tracked_time_tracker'].fillna(0,inplace=True)\n",
        "df['tracked_time_calendar'].fillna(0,inplace=True)\n",
        "df['tracked_time_timesheet'].fillna(0,inplace=True)\n",
        "\n",
        "df['days_to_purchase'].fillna(-1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpak3QHIyq7Q"
      },
      "outputs": [],
      "source": [
        "summary_statistics = df.describe(percentiles=(0.25,0.50,0.75,1)).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-jEaq-Tyuf9"
      },
      "outputs": [],
      "source": [
        "summary_statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaJ2SZZhxz5M"
      },
      "outputs": [],
      "source": [
        "# Checking distributions.\n",
        "# if Xmean = Xmedian (2nd Quartile/50%) then it is normal distribution (Gaussian).\n",
        "# if Xmean > Xmedian (2nd Quartile/50%) then it is right-skew/positive-skew distribution.\n",
        "# if Xmean < Xmedian (2nd Quartile/50%) then it is left-skew/negative-skew distribution.\n",
        "\n",
        "for column in summary_statistics:\n",
        "  if summary_statistics[column].loc['mean'] == summary_statistics[column].loc[\"50%\"]:\n",
        "    if summary_statistics[column].loc['min'] < 0:\n",
        "      print(f\"{column} data is normally distributed and has negative values.\")\n",
        "    else:\n",
        "      print(f\"{column} data is normally distributed and doesn't have negative values.\")\n",
        "  elif summary_statistics[column].loc['mean'] > summary_statistics[column].loc[\"50%\"]:\n",
        "    if summary_statistics[column].loc['min'] < 0:\n",
        "      print(f\"{column} data is positive-skew/right-skew distributed and has negative values.\")\n",
        "    else:\n",
        "      print(f\"{column} data is positive-skew/right-skew distributed and doesn't have negative values.\")\n",
        "  else:\n",
        "    if summary_statistics[column].loc['min'] < 0:\n",
        "      print(f\"{column} data is negative-skew/left-skew distributed and has negative values.\")\n",
        "    else:\n",
        "      print(f\"{column} data is negative-skew/left-skew distributed and doesn't have negative values.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzqvUY7GF7xH"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdO65ipGNEkr"
      },
      "outputs": [],
      "source": [
        "# standardization\n",
        "X = df.drop(columns=['fraud','workspace_id','continent','creation_date'])\n",
        "y = df['fraud']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "# X_train_std = (X_train - X_train.mean())/(X_train.std())\n",
        "# X_test_std = (X_test - X_train.mean())/(X_train.std())\n",
        "\n",
        "# X_train_std.dropna(axis=1, inplace=True)\n",
        "# X_test_std.dropna(axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Lqow5JQp8k"
      },
      "source": [
        "# 4.Correlation analisys\n",
        "\n",
        "In this section we use Spearmans correlation coefficient to calculate the correlation of each feature with the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt2zriOCQfFu"
      },
      "outputs": [],
      "source": [
        "correlations = {}\n",
        "p_values = {}\n",
        "\n",
        "def correlation_analysis0(data,target,coefficient):\n",
        "  for column in data.drop(target,axis=1):\n",
        "    r,p = coefficient(data[column],data[target])\n",
        "    correlations[column]=r\n",
        "    p_values[column]=p\n",
        "\n",
        "def correlation_analysis(X : pd.DataFrame, y : pd.Series, coefficient)  -> pd.DataFrame :\n",
        "  corr = {}\n",
        "  df_r = pd.DataFrame(columns = X.columns)\n",
        "  df_p = pd.DataFrame(columns = X.columns)\n",
        "  for column in X.columns:\n",
        "    r, p = coefficient(X[column],y)\n",
        "    df_r[column] = pd.Series(r)\n",
        "    df_p[column] = pd.Series(p)\n",
        "\n",
        "  df = pd.concat([df_r,df_p],axis=0)\n",
        "  df.index = ['corr','p_value']\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lyWI2saQ_EL"
      },
      "outputs": [],
      "source": [
        "# Checking correlations.\n",
        "# If your data is normally distributed, use Pearson Product-Moment Correlation.\n",
        "# If your data is skewed and you have monotonic relationship, use rank coefficients: Spearman for larger and Kendall's Tau for smaller datasets.\n",
        "from scipy.stats import pearsonr,spearmanr,kendalltau\n",
        "\n",
        "corr = correlation_analysis(X,y,coefficient = spearmanr)\n",
        "corr.round(2)\n",
        "corr.sort_values(by='corr',axis=1,ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr.sort_values(by='p_value',axis=1,ascending=False)"
      ],
      "metadata": {
        "id": "zAaUQo5FBONI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLmBR91yHgIz"
      },
      "source": [
        "#  5. Decision Tree and Feature importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WjsIi57Seqa"
      },
      "outputs": [],
      "source": [
        "#Checking the nodes structure.\n",
        "\n",
        "dec_tree_ = DecisionTreeClassifier(max_depth=4, criterion=\"entropy\",random_state=42).fit(X,y)\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(15,10))\n",
        "plot_tree(dec_tree_, feature_names=X.columns, class_names=[\"not-fraud\",\"fraud\"])\n",
        "fig.savefig(\"dec_tree.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-KSgq0ulpoa"
      },
      "outputs": [],
      "source": [
        "#Checking the nodes structure.\n",
        "\n",
        "dec_tree_ = DecisionTreeClassifier(max_depth=4, criterion=\"gini\",random_state=42).fit(X,y)\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(15,10))\n",
        "plot_tree(dec_tree_, feature_names=X.columns, class_names=[\"not-fraud\",\"fraud\"])\n",
        "fig.savefig(\"dec_tree.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ygavGqeWdzV"
      },
      "source": [
        "# 5. Information gain - feature based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaOVRvV4w173"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "print(tree.export_text(dec_tree_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtvXIz8Wysbx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def calc_entropy(x):\n",
        "    counts = np.bincount(x) # number of each unique value in a column\n",
        "    probability = counts/(len(x))\n",
        "    entropy = 0\n",
        "    for prob in probability:\n",
        "        if prob >0:\n",
        "            entropy += prob * math.log(prob, 2)\n",
        "    return -entropy\n",
        "\n",
        "def information_gain(X, column, y):\n",
        "\n",
        "    original_entropy = calc_entropy(y)\n",
        "    values = X[column].unique()\n",
        "\n",
        "    X_0 = X[X[column] == values[0]]\n",
        "    X_1 = X[X[column] == values[1]]\n",
        "\n",
        "    y_0 = y[X[column]==values[0]]\n",
        "    y_1 = y[X[column]==values[1]]\n",
        "\n",
        "    prob_0 = (X_0.shape[0])/X.shape[0]\n",
        "    prob_1 = (X_1.shape[0])/X.shape[0]\n",
        "\n",
        "    return  original_entropy - prob_0 * calc_entropy(y_0) - prob_1*calc_entropy(y_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHx7SvwY00eC"
      },
      "outputs": [],
      "source": [
        "gains_cols = dict.fromkeys(X.columns)\n",
        "for column in X.columns :\n",
        "  gains_cols[column] = (information_gain(X,column, y))\n",
        "  #print('Information gain for feature ' + column + ' equals ' + str(information_gain(X,column, y)))\n",
        "\n",
        "temp = pd.DataFrame(sorted(gains_cols.items(), key=lambda item: item[1]),columns=['feature','information_gain'])\n",
        "\n",
        "non_important5 = temp['feature'][temp['information_gain']<0.02]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKeuNZfA6mtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp.sort_values(by ='information_gain')"
      ],
      "metadata": {
        "id": "_tL1ipK36dK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4f89GwN3qrf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XC1Q7hFW5Ji"
      },
      "source": [
        "# 6. Random Forest and Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UalVgyXxttbo"
      },
      "outputs": [],
      "source": [
        "#Training RandomForestClassifier model.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(class_weight=\"balanced\",max_depth=3,criterion=\"gini\").fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD9vWl5fptW7"
      },
      "outputs": [],
      "source": [
        "feature_importances = rfc.feature_importances_\n",
        "importance_df_rf = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "importance_df_rf = importance_df_rf.sort_values(by='Importance', ascending=False)\n",
        "importance_df_rf['Cumulative Importance'] = importance_df_rf['Importance'].cumsum()\n",
        "important_features_df = importance_df_rf[importance_df_rf['Importance'] > 0.02]\n",
        "figure2 = plt.figure(figsize=(6, 4))\n",
        "plt.barh(important_features_df['Feature'], important_features_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.savefig(\"feature_importance.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZurM2Ic0iJz"
      },
      "outputs": [],
      "source": [
        "non_important6 = importance_df_rf[importance_df_rf['Importance'] < 0.02].Feature.values\n",
        "\n",
        "non_important6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzaQfhKy2Zxl"
      },
      "outputs": [],
      "source": [
        "non_important = list(set(list(non_important5.values)).intersection(list(non_important6)))\n",
        "non_important"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-JnoBpanAjuh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrIBLPMDP8ol"
      },
      "outputs": [],
      "source": [
        "# from ydata_profiling import ProfileReport\n",
        "\n",
        "# profile = ProfileReport(df, title=\"Profiling Report\")\n",
        "# profile.to_notebook_iframe()\n",
        "# profile.to_file(\"fraud_detection_exploratory_data_analysis.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Boruta Algorythm\n",
        "\n",
        "Boruta Algorythm is a feature selection algorythm that bases its decision making on the assumption that feature importance is a\n",
        "\n",
        "\n",
        "The importance of a feature of a single decision tree is calculated as the difference in performance between the model using the original features versus the model using the permuted features divided by the number of examples in the training set. The importance of a feature is the average of the measurements across all trees for that feature.\n",
        "\n",
        "Steps :\n",
        "1. Create a copy of the training set features and merges them with the original features. These synthetic features are called shadow features.\n",
        "2. Shuffle, aka, create randomized permutations of these shadow features. This is done at each iteration.\n",
        "3. Train RF and compute the z-score of all original and synthetic features.\n",
        "4. Find the maximum maximum importance of all synthetic features. A non-synthetic feature will be considered relevant if its importance is greater than this maximum.\n",
        "5. Repeat steps 2,3,4 for a number of iterations and each time take note of which original features made the cut.\n",
        "6. Use binomial distribution to finalize which features provide enough confidence to be kept in the final list."
      ],
      "metadata": {
        "id": "OQqkssIHB97p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from warnings import simplefilter\n",
        "# simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "\n",
        "# # steps 1 and 2\n",
        "# X_new = X.copy()\n",
        "# for column in X.columns:\n",
        "#     X_new[f\"shadow_{column}\"] = X_new[column].sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# def boruta_manual(X : pd.DataFrame, y : pd.Series) -> list:\n",
        "#   # steps 3,4,5\n",
        "#   rfc = RandomForestClassifier(class_weight=\"balanced\",max_depth=4,criterion=\"gini\").fit(X_new,y)\n",
        "#   importances = {feature_name: f_importance for feature_name, f_importance in zip(X_new.columns, rfc.feature_importances_)}\n",
        "#   only_shadow_feat_importance = {key:value for key,value in importances.items() if \"shadow\" in key}\n",
        "#   highest_shadow_feature = list(dict(sorted(only_shadow_feat_importance.items(), key=lambda item: item[1], reverse=True)).values())[0]\n",
        "#   selected_features = pd.DataFrame([(key, value) for key, value in importances.items() if value > highest_shadow_feature], columns = ['Feature', 'Importance'])\n",
        "#   return selected_features\n",
        "\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# TRIALS = 50\n",
        "# feature_hits = {i:0 for i in X.columns}\n",
        "# for _ in tqdm(range(TRIALS)):\n",
        "#     imp_features = boruta_manual(X, y)\n",
        "\n",
        "#     for key, _ in feature_hits.items():\n",
        "#         if key in imp_features['Feature'].values :\n",
        "#            feature_hits[key] += 1\n",
        "\n",
        "# feature_hits\n",
        "\n",
        "# # Calculate the probability mass function\n",
        "# # A binomial distribution with a probability 0.5 has a bell-shaped curve with 5% of the overall probability in the tails.\n",
        "# import scipy\n",
        "# pmf = [scipy.stats.binom.pmf(x, TRIALS, .5) for x in range(TRIALS + 1)]\n",
        "# #pmf\n",
        "\n",
        "# def get_tail_items(pmf):\n",
        "#   total = 0\n",
        "#   for i, x in enumerate(pmf):\n",
        "#     total += x\n",
        "#     if total >= 0.05:\n",
        "#       break\n",
        "#   return i\n",
        "\n",
        "# plt.plot([i for i in range(TRIALS + 1)], pmf,\"-o\")\n",
        "# plt.title(f\"Binomial distribution for {TRIALS} trials\")\n",
        "# plt.xlabel(\"No. of trials\")\n",
        "# plt.ylabel(\"Probability\")\n",
        "# plt.grid(True)\n",
        "\n",
        "\n",
        "# # select features from n number of trials\n",
        "# def choose_features(feature_hits, TRIALS, thresh):\n",
        "#   #define boundries\n",
        "#   green_zone_thresh = TRIALS - thresh\n",
        "#   blue_zone_upper = green_zone_thresh\n",
        "#   blue_zone_lower = thresh\n",
        "\n",
        "#   green_zone = [key for key, value in feature_hits.items() if value >= green_zone_thresh]\n",
        "#   blue_zone = [key for key, value in feature_hits.items() if (value >= blue_zone_lower and value < blue_zone_upper)]\n",
        "\n",
        "#   return green_zone, blue_zone\n",
        "\n",
        "# thresh = get_tail_items(pmf)\n",
        "# green, blue = choose_features(feature_hits, TRIALS, thresh)"
      ],
      "metadata": {
        "id": "dte3Y14jB8hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet  boruta\n"
      ],
      "metadata": {
        "id": "tcQ0jsHotBz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from boruta import boruta_py\n",
        "\n",
        "with open('boruta_py.txt','r') as writer_file:\n",
        "    contents_to_write = writer_file.read()\n",
        "with open(boruta_py.__file__,'w') as file_to_overwrite:\n",
        "    file_to_overwrite.write(contents_to_write)\n",
        "rfc = RandomForestClassifier(class_weight=\"balanced\",max_depth=4,criterion=\"gini\").fit(X,y)\n",
        "\n",
        "\n",
        "from boruta import BorutaPy\n",
        "# let's initialize Boruta\n",
        "feat_selector = BorutaPy(\n",
        "    verbose=2,\n",
        "    estimator=rfc,\n",
        "    n_estimators='auto',\n",
        "    max_iter=10  # number of iterations to perform\n",
        ")\n",
        "\n",
        "# train Boruta\n",
        "# N.B.: X and y must be numpy arrays\n",
        "x_arr = np.array(X)\n",
        "y_arr = np.array(y)\n",
        "feat_selector.fit(x_arr, y_arr)\n",
        "\n",
        "# # print support and ranking for each feature\n",
        "# print(\"\\n------Support and Ranking for each feature------\")\n",
        "# for i in range(len(feat_selector.support_)):\n",
        "#     if feat_selector.support_[i]:\n",
        "#         print(\"Passes the test: \", X.columns[i],\n",
        "#               \" - Ranking: \", feat_selector.ranking_[i])\n",
        "#     else:\n",
        "#         print(\"Doesn't pass the test: \",\n",
        "#               X.columns[i], \" - Ranking: \", feat_selector.ranking_[i])\n",
        "\n",
        "\n",
        "\n",
        "number_important = np.sum(feat_selector.support_ + feat_selector.support_weak_)\n",
        "all_features = len(X.columns)\n",
        "very_important = sum(feat_selector.support_)\n",
        "\n",
        "print(f'Number of features:{all_features}')\n",
        "print(f'Number of important features:{number_important} - out of which {very_important} very important features.')\n",
        "print(pd.Series(feat_selector.ranking_[feat_selector.support_], index=X.columns[feat_selector.support_], name='Boruta Analysis - Supported'))\n",
        "print(pd.Series(feat_selector.ranking_[feat_selector.support_weak_], index=X.columns[feat_selector.support_weak_], name='Boruta Analysis - Weak'))\n",
        "print(pd.Series(feat_selector.ranking_[1 - (feat_selector.support_weak_+feat_selector.support_)], index=X.columns[1 - (feat_selector.support_weak_+feat_selector.support_)], name='Boruta Analysis - Not supported'))\n",
        "\n"
      ],
      "metadata": {
        "id": "hSu_Ynufsxmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyCaret"
      ],
      "metadata": {
        "id": "cDEc1CCmDUDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkHYM58E4gOb"
      },
      "outputs": [],
      "source": [
        "from pycaret.classification import *\n",
        "import seaborn as sns\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "# Checking the models: hyperparameters.\n",
        "\n",
        "s = setup(data = df, target='fraud', session_id=42)\n",
        "\n",
        "best = compare_models()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}